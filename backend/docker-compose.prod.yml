services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: fairpath-backend-prod
    ports:
      - "${PORT:-8000}:8000"
    env_file:
      - .env
    environment:
      - ENV_MODE=production
      - PORT=${PORT:-8000}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - CORS_ORIGINS=${CORS_ORIGINS:-https://fair-path.vercel.app,http://localhost:3000,http://localhost:5173}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-60}
      - RATE_LIMIT_PER_HOUR=${RATE_LIMIT_PER_HOUR:-1000}
      - RATE_LIMIT_PER_DAY=${RATE_LIMIT_PER_DAY:-10000}
      - MAX_REQUEST_SIZE=${MAX_REQUEST_SIZE:-1048576}
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-10485760}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-30}
      - OPENAI_MAX_RETRIES=${OPENAI_MAX_RETRIES:-2}
      - OPENAI_RETRY_DELAY=${OPENAI_RETRY_DELAY:-1.0}
    volumes:
      # Mount artifacts directory for model persistence
      - ./artifacts:/app/artifacts
      # Mount data directory for data persistence
      - ./data:/app/data
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    networks:
      - fairpath-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  fairpath-network:
    driver: bridge

